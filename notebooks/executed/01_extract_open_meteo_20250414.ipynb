{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adfa3fd7-cf37-4e76-955c-0a5b219d4833",
   "metadata": {
    "papermill": {
     "duration": 0.009907,
     "end_time": "2025-04-14T23:19:38.390540",
     "exception": false,
     "start_time": "2025-04-14T23:19:38.380633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üì° Data Extraction ‚Äî Open-Meteo API\n",
    "\n",
    "Este notebook realiza a **extra√ß√£o de dados clim√°ticos** da API p√∫blica [Open-Meteo](https://open-meteo.com/).  \n",
    "Os dados s√£o referentes a previs√µes di√°rias de temperatura para os **pr√≥ximos 90 dias** em cidades da **Am√©rica do Sul**.\n",
    "\n",
    "üìå Etapa: Extra√ß√£o  \n",
    "üíæ Destino: Camada Bronze do Data Lake (formato Delta)  \n",
    "üåé Cidades: S√£o Paulo, Buenos Aires, Lima, Santiago, Bogot√°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2d1f98-b645-43e6-9621-dce851f82d48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T23:19:38.403095Z",
     "iopub.status.busy": "2025-04-14T23:19:38.402657Z",
     "iopub.status.idle": "2025-04-14T23:19:38.918280Z",
     "shell.execute_reply": "2025-04-14T23:19:38.917197Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.522078,
     "end_time": "2025-04-14T23:19:38.920054",
     "exception": false,
     "start_time": "2025-04-14T23:19:38.397976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports principais: bibliotecas padr√£o e PySpark\n",
    "import requests  # Para chamadas HTTP\n",
    "import pandas as pd  # Para manipula√ß√£o tabular com DataFrame\n",
    "from pyspark.sql import SparkSession  # Para iniciar a SparkSession\n",
    "from delta import configure_spark_with_delta_pip  # Para usar Delta Lake\n",
    "from datetime import datetime, timedelta, date  # Para trabalhar com datas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130e264a-918d-4d2d-9843-55ca0562cb9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T23:19:38.925161Z",
     "iopub.status.busy": "2025-04-14T23:19:38.924732Z",
     "iopub.status.idle": "2025-04-14T23:19:43.264096Z",
     "shell.execute_reply": "2025-04-14T23:19:43.263308Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 4.344564,
     "end_time": "2025-04-14T23:19:43.266196",
     "exception": false,
     "start_time": "2025-04-14T23:19:38.921632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/14 20:19:40 WARN Utils: Your hostname, obi-wan-kenote resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/14 20:19:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/kenote_ubuntu/projetos/Airflow/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/kenote_ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/kenote_ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a4c697e0-ca59-441c-a089-1d9c3439ae3c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 159ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a4c697e0-ca59-441c-a089-1d9c3439ae3c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/14 20:19:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Criando a SparkSession com suporte ao Delta Lake (Delta Spark)\n",
    "\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeIngestion\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba25d5c2-290a-46fa-99d4-6ebe6c29b6a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T23:19:43.272956Z",
     "iopub.status.busy": "2025-04-14T23:19:43.272704Z",
     "iopub.status.idle": "2025-04-14T23:19:43.277694Z",
     "shell.execute_reply": "2025-04-14T23:19:43.276966Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.010402,
     "end_time": "2025-04-14T23:19:43.278818",
     "exception": false,
     "start_time": "2025-04-14T23:19:43.268416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lista de cidades com suas coordenadas geogr√°ficas\n",
    "# Fonte: Google Maps ou dados oficiais da API\n",
    "locations = [\n",
    "    {\"city\": \"Cabedelo\", \"lat\": -6.97, \"lon\": -34.83},\n",
    "    {\"city\": \"Jo√£o Pessoa\", \"lat\": -7.12, \"lon\": -34.87},\n",
    "    {\"city\": \"S√£o Paulo\", \"lat\": -23.55, \"lon\": -46.63},\n",
    "    {\"city\": \"Buenos Aires\", \"lat\": -34.60, \"lon\": -58.38},\n",
    "    {\"city\": \"Lima\", \"lat\": -12.04, \"lon\": -77.03},\n",
    "    {\"city\": \"Santiago\", \"lat\": -33.45, \"lon\": -70.66},\n",
    "    {\"city\": \"Bogot√°\", \"lat\": 4.71, \"lon\": -74.07}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12f5b9a-0f8b-4cfc-9a1c-190e91aec582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T23:19:43.285683Z",
     "iopub.status.busy": "2025-04-14T23:19:43.285414Z",
     "iopub.status.idle": "2025-04-14T23:19:53.661469Z",
     "shell.execute_reply": "2025-04-14T23:19:53.659062Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 10.383513,
     "end_time": "2025-04-14T23:19:53.664491",
     "exception": false,
     "start_time": "2025-04-14T23:19:43.280978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Coletando dados de 2025-04-14 at√© 2025-04-29\n",
      "\n",
      "üîç Testando URL para Cabedelo:\n",
      "https://api.open-meteo.com/v1/forecast?latitude=-6.97&longitude=-34.83&daily=temperature_2m_max,temperature_2m_min&timezone=auto&start_date=2025-04-14&end_date=2025-04-29\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Resposta da API (200) para Cabedelo:\n",
      "{\"latitude\":-7.0,\"longitude\":-34.875,\"generationtime_ms\":0.03612041473388672,\"utc_offset_seconds\":-10800,\"timezone\":\"America/Fortaleza\",\"timezone_abbreviation\":\"GMT-3\",\"elevation\":6.0,\"daily_units\":{\"...\n",
      "\n",
      "‚úÖ Dados coletados com sucesso para: Cabedelo\n",
      "\n",
      "üîç Testando URL para Jo√£o Pessoa:\n",
      "https://api.open-meteo.com/v1/forecast?latitude=-7.12&longitude=-34.87&daily=temperature_2m_max,temperature_2m_min&timezone=auto&start_date=2025-04-14&end_date=2025-04-29\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Resposta da API (200) para Jo√£o Pessoa:\n",
      "{\"latitude\":-7.125,\"longitude\":-34.875,\"generationtime_ms\":0.04100799560546875,\"utc_offset_seconds\":-10800,\"timezone\":\"America/Fortaleza\",\"timezone_abbreviation\":\"GMT-3\",\"elevation\":43.0,\"daily_units\"...\n",
      "\n",
      "‚úÖ Dados coletados com sucesso para: Jo√£o Pessoa\n",
      "\n",
      "üîç Testando URL para S√£o Paulo:\n",
      "https://api.open-meteo.com/v1/forecast?latitude=-23.55&longitude=-46.63&daily=temperature_2m_max,temperature_2m_min&timezone=auto&start_date=2025-04-14&end_date=2025-04-29\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Resposta da API (200) para S√£o Paulo:\n",
      "{\"latitude\":-23.5,\"longitude\":-46.5,\"generationtime_ms\":0.036597251892089844,\"utc_offset_seconds\":-10800,\"timezone\":\"America/Sao_Paulo\",\"timezone_abbreviation\":\"GMT-3\",\"elevation\":737.0,\"daily_units\":...\n",
      "\n",
      "‚úÖ Dados coletados com sucesso para: S√£o Paulo\n",
      "\n",
      "üîç Testando URL para Buenos Aires:\n",
      "https://api.open-meteo.com/v1/forecast?latitude=-34.6&longitude=-58.38&daily=temperature_2m_max,temperature_2m_min&timezone=auto&start_date=2025-04-14&end_date=2025-04-29\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Resposta da API (200) para Buenos Aires:\n",
      "{\"latitude\":-34.625,\"longitude\":-58.5,\"generationtime_ms\":0.035643577575683594,\"utc_offset_seconds\":-10800,\"timezone\":\"America/Argentina/Buenos_Aires\",\"timezone_abbreviation\":\"GMT-3\",\"elevation\":23.0,...\n",
      "\n",
      "‚úÖ Dados coletados com sucesso para: Buenos Aires\n",
      "\n",
      "üîç Testando URL para Lima:\n",
      "https://api.open-meteo.com/v1/forecast?latitude=-12.04&longitude=-77.03&daily=temperature_2m_max,temperature_2m_min&timezone=auto&start_date=2025-04-14&end_date=2025-04-29\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Resposta da API (200) para Lima:\n",
      "{\"latitude\":-11.875,\"longitude\":-77.125,\"generationtime_ms\":0.04017353057861328,\"utc_offset_seconds\":-18000,\"timezone\":\"America/Lima\",\"timezone_abbreviation\":\"GMT-5\",\"elevation\":152.0,\"daily_units\":{\"...\n",
      "\n",
      "‚úÖ Dados coletados com sucesso para: Lima\n",
      "\n",
      "üîç Testando URL para Santiago:\n",
      "https://api.open-meteo.com/v1/forecast?latitude=-33.45&longitude=-70.66&daily=temperature_2m_max,temperature_2m_min&timezone=auto&start_date=2025-04-14&end_date=2025-04-29\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Resposta da API (200) para Santiago:\n",
      "{\"latitude\":-33.5,\"longitude\":-70.625,\"generationtime_ms\":0.031948089599609375,\"utc_offset_seconds\":-14400,\"timezone\":\"America/Santiago\",\"timezone_abbreviation\":\"GMT-4\",\"elevation\":541.0,\"daily_units\"...\n",
      "\n",
      "‚úÖ Dados coletados com sucesso para: Santiago\n",
      "\n",
      "üîç Testando URL para Bogot√°:\n",
      "https://api.open-meteo.com/v1/forecast?latitude=4.71&longitude=-74.07&daily=temperature_2m_max,temperature_2m_min&timezone=auto&start_date=2025-04-14&end_date=2025-04-29\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Resposta da API (200) para Bogot√°:\n",
      "{\"latitude\":4.875,\"longitude\":-74.25,\"generationtime_ms\":0.052809715270996094,\"utc_offset_seconds\":-18000,\"timezone\":\"America/Bogota\",\"timezone_abbreviation\":\"GMT-5\",\"elevation\":2558.0,\"daily_units\":{...\n",
      "\n",
      "‚úÖ Dados coletados com sucesso para: Bogot√°\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, date\n",
    "import requests\n",
    "\n",
    "# 1. Define as datas v√°lidas\n",
    "start_date = datetime.today().date()\n",
    "max_forecast_date = date(2025, 4, 29)\n",
    "end_date = min(start_date + timedelta(days=89), max_forecast_date)\n",
    "\n",
    "# 2. Converte para string no formato YYYY-MM-DD\n",
    "start_date_str = start_date.isoformat()\n",
    "end_date_str = end_date.isoformat()\n",
    "\n",
    "print(f\"üìÖ Coletando dados de {start_date_str} at√© {end_date_str}\\n\")\n",
    "\n",
    "# 3. Lista para armazenar os dados\n",
    "raw_data = []\n",
    "\n",
    "# 4. Faz a requisi√ß√£o para cada cidade\n",
    "for loc in locations:\n",
    "    url = (\n",
    "        f\"https://api.open-meteo.com/v1/forecast\"\n",
    "        f\"?latitude={loc['lat']}&longitude={loc['lon']}\"\n",
    "        f\"&daily=temperature_2m_max,temperature_2m_min\"\n",
    "        f\"&timezone=auto\"\n",
    "        f\"&start_date={start_date_str}&end_date={end_date_str}\"\n",
    "    )\n",
    "\n",
    "    print(f\"üîç Testando URL para {loc['city']}:\\n{url}\\n\")\n",
    "\n",
    "    response = requests.get(url)\n",
    "    print(f\"üì® Resposta da API ({response.status_code}) para {loc['city']}:\\n{response.text[:200]}...\\n\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json = response.json()\n",
    "        if \"daily\" in json and \"time\" in json[\"daily\"]:\n",
    "            for i in range(len(json[\"daily\"][\"time\"])):\n",
    "                raw_data.append({\n",
    "                    \"city\": loc[\"city\"],\n",
    "                    \"date\": json[\"daily\"][\"time\"][i],\n",
    "                    \"temp_min\": json[\"daily\"][\"temperature_2m_min\"][i],\n",
    "                    \"temp_max\": json[\"daily\"][\"temperature_2m_max\"][i],\n",
    "                })\n",
    "            print(f\"‚úÖ Dados coletados com sucesso para: {loc['city']}\\n\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Dados ausentes no corpo da resposta para: {loc['city']}\\n\")\n",
    "    else:\n",
    "        print(f\"‚ùå Erro ao acessar a API para: {loc['city']} ‚Äî status: {response.status_code}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4beea55-acf1-4bfb-9981-eaf89669da04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T23:19:53.674501Z",
     "iopub.status.busy": "2025-04-14T23:19:53.673803Z",
     "iopub.status.idle": "2025-04-14T23:19:55.938887Z",
     "shell.execute_reply": "2025-04-14T23:19:55.937444Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 2.272952,
     "end_time": "2025-04-14T23:19:55.941180",
     "exception": false,
     "start_time": "2025-04-14T23:19:53.668228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenote_ubuntu/projetos/Airflow/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/kenote_ubuntu/projetos/Airflow/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "# Convertendo os dados para um DataFrame Pandas\n",
    "df_pd = pd.DataFrame(raw_data)\n",
    "\n",
    "# Convertendo o DataFrame Pandas para DataFrame Spark\n",
    "df_spark = spark.createDataFrame(df_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8424eb88-db19-4d18-99fd-08bce94f1be2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T23:19:55.950967Z",
     "iopub.status.busy": "2025-04-14T23:19:55.950006Z",
     "iopub.status.idle": "2025-04-14T23:20:11.179991Z",
     "shell.execute_reply": "2025-04-14T23:20:11.178577Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 15.235781,
     "end_time": "2025-04-14T23:20:11.181788",
     "exception": false,
     "start_time": "2025-04-14T23:19:55.946007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/14 20:20:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/04/14 20:20:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/04/14 20:20:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/04/14 20:20:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/04/14 20:20:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/14 20:20:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/04/14 20:20:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/04/14 20:20:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/04/14 20:20:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/14 20:20:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                        (0 + 12) / 50]\r",
      "\r",
      "[Stage 4:======>                                                  (6 + 12) / 50]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:=============>                                          (12 + 12) / 50]\r",
      "\r",
      "[Stage 4:=========================>                              (23 + 12) / 50]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:====================================>                   (33 + 12) / 50]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:======================>                                (20 + 13) / 50]\r",
      "\r",
      "[Stage 13:===================================================>    (46 + 4) / 50]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Caminho da camada Bronze\n",
    "bronze_path = \"/home/kenote_ubuntu/projetos/Airflow/data/bronze/open_meteo\"\n",
    "\n",
    "# Salvando os dados brutos no formato Delta\n",
    "df_spark.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4b224a-887c-4596-80fb-f37cb362e894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T23:20:11.191411Z",
     "iopub.status.busy": "2025-04-14T23:20:11.191166Z",
     "iopub.status.idle": "2025-04-14T23:20:11.744465Z",
     "shell.execute_reply": "2025-04-14T23:20:11.743482Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.558608,
     "end_time": "2025-04-14T23:20:11.746129",
     "exception": false,
     "start_time": "2025-04-14T23:20:11.187521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>date</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bogot√°</td>\n",
       "      <td>2025-04-17</td>\n",
       "      <td>11.6</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bogot√°</td>\n",
       "      <td>2025-04-18</td>\n",
       "      <td>13.4</td>\n",
       "      <td>21.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bogot√°</td>\n",
       "      <td>2025-04-19</td>\n",
       "      <td>13.1</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bogot√°</td>\n",
       "      <td>2025-04-20</td>\n",
       "      <td>13.3</td>\n",
       "      <td>19.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bogot√°</td>\n",
       "      <td>2025-04-21</td>\n",
       "      <td>11.5</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bogot√°</td>\n",
       "      <td>2025-04-22</td>\n",
       "      <td>10.6</td>\n",
       "      <td>18.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bogot√°</td>\n",
       "      <td>2025-04-23</td>\n",
       "      <td>11.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bogot√°</td>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>10.6</td>\n",
       "      <td>16.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bogot√°</td>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>9.8</td>\n",
       "      <td>14.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bogot√°</td>\n",
       "      <td>2025-04-26</td>\n",
       "      <td>9.2</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     city        date  temp_min  temp_max\n",
       "0  Bogot√°  2025-04-17      11.6      20.6\n",
       "1  Bogot√°  2025-04-18      13.4      21.3\n",
       "2  Bogot√°  2025-04-19      13.1      21.6\n",
       "3  Bogot√°  2025-04-20      13.3      19.3\n",
       "4  Bogot√°  2025-04-21      11.5      19.0\n",
       "5  Bogot√°  2025-04-22      10.6      18.1\n",
       "6  Bogot√°  2025-04-23      11.0      18.0\n",
       "7  Bogot√°  2025-04-24      10.6      16.3\n",
       "8  Bogot√°  2025-04-25       9.8      14.9\n",
       "9  Bogot√°  2025-04-26       9.2      18.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Releitura do arquivo salvo para validar a persist√™ncia no Delta\n",
    "df_read = spark.read.format(\"delta\").load(bronze_path)\n",
    "\n",
    "# Visualiza√ß√£o final para garantir que est√° tudo certo\n",
    "df_read.toPandas().head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d3670-1c01-42af-9645-da2f95768219",
   "metadata": {
    "papermill": {
     "duration": 0.002877,
     "end_time": "2025-04-14T23:20:11.751609",
     "exception": false,
     "start_time": "2025-04-14T23:20:11.748732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusao\n",
    "\n",
    "‚úÖ Extra√ß√£o de dados de clima da API Open-Meteo, salvando os dados crus na camada Bronze do Data Lake, mantendo:\n",
    "\n",
    "üì¶ Dados n√£o transformados\n",
    "\n",
    "‚úÖ Arquivo Delta version√°vel\n",
    "\n",
    "üß™ Visualiza√ß√µes pr√© e p√≥s salvamento\n",
    "\n",
    "‚ú® Organiza√ß√£o e documenta√ß√£o ideal para projetos profissionais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36.851384,
   "end_time": "2025-04-14T23:20:14.376938",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/kenote_ubuntu/projetos/Airflow/notebooks/extraction/01_extract_open_meteo.ipynb",
   "output_path": "/home/kenote_ubuntu/projetos/Airflow/notebooks/executed/01_extract_open_meteo_20250414.ipynb",
   "parameters": {},
   "start_time": "2025-04-14T23:19:37.525554",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77471ca7-d8a2-4e4c-9723-ddf6e7b9c639",
   "metadata": {},
   "source": [
    "# Transformação da Camada Silver — Open Meteo\n",
    "Neste notebook transformamos os dados brutos da camada Bronze em dados limpos e prontos para análise, compondo a camada Silver do Data Lake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce424a1f-1587-45b6-ac69-817ce61538cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d2feca-60cd-4c82-8e86-06294d10ae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 15:44:58 WARN Utils: Your hostname, obi-wan-kenote resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/14 15:44:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/kenote_ubuntu/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/kenote_ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/kenote_ubuntu/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e98508bc-097e-43c3-bd46-f66306eea2e3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.0 in central\n",
      "\tfound io.delta#delta-storage;3.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 169ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e98508bc-097e-43c3-bd46-f66306eea2e3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/8ms)\n",
      "25/04/14 15:44:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/14 15:45:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/14 15:45:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder \\\n",
    "    .appName(\"SilverTransformOpenMeteo\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c41904-cd6a-4600-9fc4-113e40dfa962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 15:45:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+--------+\n",
      "|        city|      date|temp_min|temp_max|\n",
      "+------------+----------+--------+--------+\n",
      "|Buenos Aires|2025-04-20|     8.5|    18.1|\n",
      "|Buenos Aires|2025-04-21|    11.2|    18.7|\n",
      "|Buenos Aires|2025-04-22|    14.2|    19.9|\n",
      "|Buenos Aires|2025-04-23|    15.4|    22.8|\n",
      "|Buenos Aires|2025-04-24|    17.0|    19.9|\n",
      "+------------+----------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Caminho absoluto da camada Bronze\n",
    "bronze_path = \"/home/kenote_ubuntu/projetos/Airflow/data/bronze/open_meteo\"\n",
    "\n",
    "# Leitura dos dados brutos (Delta)\n",
    "df_bronze = spark.read.format(\"delta\").load(bronze_path)\n",
    "\n",
    "# Visualizando as primeiras linhas\n",
    "df_bronze.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55eb2224-edff-4969-8f8a-24cd94d8b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeando colunas para padrão snake_case (caso necessário)\n",
    "df_silver = df_bronze.select(\n",
    "    col(\"city\").alias(\"city\"),\n",
    "    col(\"date\").alias(\"date\"),\n",
    "    col(\"temp_min\").alias(\"temperature_min_c\"),\n",
    "    col(\"temp_max\").alias(\"temperature_max_c\")\n",
    ")\n",
    "\n",
    "# Conversão de tipos, se necessário\n",
    "# (Spark já infere corretamente, mas vamos garantir que temperatura seja float)\n",
    "df_silver = df_silver.withColumn(\"temperature_min_c\", col(\"temperature_min_c\").cast(\"double\"))\n",
    "df_silver = df_silver.withColumn(\"temperature_max_c\", col(\"temperature_max_c\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5cb6c9-b09d-4c0c-ae98-40170de74d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----------------+-----------------+\n",
      "|        city|      date|temperature_min_c|temperature_max_c|\n",
      "+------------+----------+-----------------+-----------------+\n",
      "|Buenos Aires|2025-04-20|              8.5|             18.1|\n",
      "|Buenos Aires|2025-04-21|             11.2|             18.7|\n",
      "|Buenos Aires|2025-04-22|             14.2|             19.9|\n",
      "|Buenos Aires|2025-04-23|             15.4|             22.8|\n",
      "|Buenos Aires|2025-04-24|             17.0|             19.9|\n",
      "|Buenos Aires|2025-04-25|             16.3|             19.6|\n",
      "|Buenos Aires|2025-04-26|             15.8|             19.6|\n",
      "|Buenos Aires|2025-04-27|             16.5|             20.1|\n",
      "|Buenos Aires|2025-04-28|             17.0|             23.2|\n",
      "|        Lima|2025-04-22|             19.0|             22.1|\n",
      "+------------+----------+-----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- temperature_min_c: double (nullable = true)\n",
      " |-- temperature_max_c: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#visualização dos dados tratados\n",
    "df_silver.show(10)\n",
    "df_silver.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b2bbd2e-cfed-4c53-a1c7-cf3e45f28f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 15:45:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/04/14 15:45:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/04/14 15:45:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/04/14 15:45:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/04/14 15:45:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/04/14 15:45:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/04/14 15:45:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/04/14 15:45:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/04/14 15:45:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "#Salvando a camada silves como Delta\n",
    "\n",
    "# Caminho absoluto da camada Silver\n",
    "silver_path = \"/home/kenote_ubuntu/projetos/Airflow/data/silver/open_meteo\"\n",
    "\n",
    "# Salvando os dados limpos no formato Delta\n",
    "df_silver.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca2b70e-a78c-4f2b-8826-572d88abf767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----------------+-----------------+\n",
      "|        city|      date|temperature_min_c|temperature_max_c|\n",
      "+------------+----------+-----------------+-----------------+\n",
      "|   São Paulo|2025-04-27|             15.8|             24.8|\n",
      "|   São Paulo|2025-04-28|             17.0|             26.1|\n",
      "|   São Paulo|2025-04-29|             18.0|             20.0|\n",
      "|Buenos Aires|2025-04-14|             10.7|             21.4|\n",
      "|Buenos Aires|2025-04-15|             14.5|             19.6|\n",
      "+------------+----------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- temperature_min_c: double (nullable = true)\n",
      " |-- temperature_max_c: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verificando os dados salvos\n",
    "\n",
    "# Lendo novamente para validar o conteúdo da Silver\n",
    "df_check = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# Exibindo os dados para garantir que tudo está correto\n",
    "df_check.show(5)\n",
    "df_check.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ddb42-bb45-49e7-98ce-a0c3a0379dac",
   "metadata": {},
   "source": [
    "# Conclusao\n",
    "\n",
    "✅ Leitura dos dados da Bronze (caminho absoluto)\n",
    "✅ Transformações de limpeza e padronização\n",
    "✅ Salvar resultado da Silver como Delta\n",
    "✅ Comentários bem didáticos\n",
    "✅ Verificação final do conteúdo salvo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

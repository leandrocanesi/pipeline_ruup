{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6121971-ed52-4172-b4af-0317134ad6db",
   "metadata": {},
   "source": [
    "# Prepara√ß√£o da Camada Gold ‚Äî Open Meteo\n",
    "Consolida√ß√£o dos dados limpos da camada Silver em uma estrutura anal√≠tica pronta para consumo por dashboards e ferramentas de BI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ccf6089-fe86-454c-8c36-b2cef3f17408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql.functions import col, avg, min, max, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af48011-693e-42de-810b-abbb7aa23d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 15:46:18 WARN Utils: Your hostname, obi-wan-kenote resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/14 15:46:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/kenote_ubuntu/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/kenote_ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/kenote_ubuntu/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c865f4c8-f9fd-4731-af32-6322e3e817c3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.0 in central\n",
      "\tfound io.delta#delta-storage;3.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 145ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c865f4c8-f9fd-4731-af32-6322e3e817c3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "25/04/14 15:46:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/14 15:46:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/14 15:46:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/04/14 15:46:20 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder \\\n",
    "    .appName(\"GoldConsolidationOpenMeteo\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebbabe58-b2fc-469c-9fc9-0abdd61c2773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 15:46:26 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----------------+-----------------+\n",
      "|        city|      date|temperature_min_c|temperature_max_c|\n",
      "+------------+----------+-----------------+-----------------+\n",
      "|   S√£o Paulo|2025-04-27|             15.8|             24.8|\n",
      "|   S√£o Paulo|2025-04-28|             17.0|             26.1|\n",
      "|   S√£o Paulo|2025-04-29|             18.0|             20.0|\n",
      "|Buenos Aires|2025-04-14|             10.7|             21.4|\n",
      "|Buenos Aires|2025-04-15|             14.5|             19.6|\n",
      "+------------+----------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Leitura da Camada Silver\n",
    "\n",
    "# Caminho absoluto da camada Silver\n",
    "silver_path = \"/home/kenote_ubuntu/projetos/Airflow/data/silver/open_meteo\"\n",
    "\n",
    "# Lendo os dados tratados da Silver\n",
    "df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# Visualizando os dados\n",
    "df_silver.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31135f8-05a8-4465-a4cd-2b1b137fb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agrega√ß√µes para consolidar\n",
    "\n",
    "# Consolidando por cidade: m√©dia, m√≠nimo, m√°ximo de temperaturas\n",
    "df_gold = df_silver.groupBy(\"city\").agg(\n",
    "    count(\"*\").alias(\"days_count\"),\n",
    "    avg(\"temperature_min_c\").alias(\"avg_temp_min_c\"),\n",
    "    min(\"temperature_min_c\").alias(\"min_temp_min_c\"),\n",
    "    max(\"temperature_min_c\").alias(\"max_temp_min_c\"),\n",
    "    avg(\"temperature_max_c\").alias(\"avg_temp_max_c\"),\n",
    "    min(\"temperature_max_c\").alias(\"min_temp_max_c\"),\n",
    "    max(\"temperature_max_c\").alias(\"max_temp_max_c\")\n",
    ")\n",
    "\n",
    "# Arredondando (opcional, apenas visual)\n",
    "df_gold = df_gold.select(\n",
    "    \"city\", \"days_count\",\n",
    "    col(\"avg_temp_min_c\").cast(\"decimal(5,2)\"),\n",
    "    col(\"min_temp_min_c\").cast(\"double\"),\n",
    "    col(\"max_temp_min_c\").cast(\"double\"),\n",
    "    col(\"avg_temp_max_c\").cast(\"decimal(5,2)\"),\n",
    "    col(\"min_temp_max_c\").cast(\"double\"),\n",
    "    col(\"max_temp_max_c\").cast(\"double\")\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d216c1a2-25fb-4402-80de-78d3703d66c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|city        |days_count|avg_temp_min_c|min_temp_min_c|max_temp_min_c|avg_temp_max_c|min_temp_max_c|max_temp_max_c|\n",
      "+------------+----------+--------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|Bogot√°      |16        |11.44         |9.2           |13.3          |18.51         |13.2          |21.5          |\n",
      "|Buenos Aires|16        |13.94         |8.5           |17.0          |20.41         |18.1          |23.2          |\n",
      "|Cabedelo    |16        |25.94         |25.0          |26.8          |28.96         |28.2          |30.1          |\n",
      "|Jo√£o Pessoa |16        |23.61         |21.7          |25.6          |30.19         |29.1          |31.3          |\n",
      "|S√£o Paulo   |16        |16.61         |14.3          |18.5          |22.23         |17.0          |26.1          |\n",
      "|Santiago    |16        |14.26         |9.5           |19.1          |25.19         |20.9          |30.0          |\n",
      "|Lima        |16        |17.94         |17.0          |19.0          |23.51         |20.7          |26.9          |\n",
      "+------------+----------+--------------+--------------+--------------+--------------+--------------+--------------+\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- days_count: long (nullable = false)\n",
      " |-- avg_temp_min_c: decimal(5,2) (nullable = true)\n",
      " |-- min_temp_min_c: double (nullable = true)\n",
      " |-- max_temp_min_c: double (nullable = true)\n",
      " |-- avg_temp_max_c: decimal(5,2) (nullable = true)\n",
      " |-- min_temp_max_c: double (nullable = true)\n",
      " |-- max_temp_max_c: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Visualiza√ß√£o r√°pida da gold\n",
    "\n",
    "df_gold.show(truncate=False)\n",
    "df_gold.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "460a0731-da94-4aaa-85d0-485cc39d3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando a camada Gold como Delta\n",
    "\n",
    "# Caminho absoluto da camada Gold\n",
    "gold_path = \"/home/kenote_ubuntu/projetos/Airflow/data/gold/open_meteo\"\n",
    "\n",
    "# Salvando no formato Delta\n",
    "df_gold.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(gold_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a5948f-a14f-4b78-8cf9-88f721919811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|city        |days_count|avg_temp_min_c|min_temp_min_c|max_temp_min_c|avg_temp_max_c|min_temp_max_c|max_temp_max_c|\n",
      "+------------+----------+--------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|Bogot√°      |16        |11.44         |9.2           |13.3          |18.51         |13.2          |21.5          |\n",
      "|Buenos Aires|16        |13.94         |8.5           |17.0          |20.41         |18.1          |23.2          |\n",
      "|Cabedelo    |16        |25.94         |25.0          |26.8          |28.96         |28.2          |30.1          |\n",
      "|Jo√£o Pessoa |16        |23.61         |21.7          |25.6          |30.19         |29.1          |31.3          |\n",
      "|S√£o Paulo   |16        |16.61         |14.3          |18.5          |22.23         |17.0          |26.1          |\n",
      "|Santiago    |16        |14.26         |9.5           |19.1          |25.19         |20.9          |30.0          |\n",
      "|Lima        |16        |17.94         |17.0          |19.0          |23.51         |20.7          |26.9          |\n",
      "+------------+----------+--------------+--------------+--------------+--------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verificando os dados salvos\n",
    "\n",
    "# Lendo novamente para validar o conte√∫do da gold\n",
    "df_check = spark.read.format(\"delta\").load(gold_path)\n",
    "\n",
    "# Conferindo o resultado final salvo\n",
    "df_check.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44d22d-b00c-42cc-bfa6-3af647de8de4",
   "metadata": {},
   "source": [
    "Conclusao\n",
    "üîç Leitura da silver Confirma que os dados foram salvos corretamente ‚úÖ Agrupamento por cidade üö® M√©dia, m√≠nimo, m√°ximo de temperaturar üìä Salvamento no Delta üìà Visualiza√ß√£o Comunica√ß√£o clara para quem for usar os dados depois üßº Pronto pra consumo em dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
